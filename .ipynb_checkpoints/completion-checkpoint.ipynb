{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6899e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fcc3a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b11e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trains 23547\n",
      "node_ingredients 6714\n",
      "val_cpt_q 7848\n",
      "val_cpt_a 7848\n"
     ]
    }
   ],
   "source": [
    "# prepare dataset\n",
    "\n",
    "f = open('train.csv', 'r', encoding='utf-8')\n",
    "reader = csv.reader(f)\n",
    "trains = []\n",
    "for line in reader:\n",
    "    trains.append(line)\n",
    "f.close()    \n",
    "print(\"trains\", len(trains))\n",
    "\n",
    "f = open('node_ingredient.csv', 'r', encoding='utf-8')\n",
    "reader = csv.reader(f)\n",
    "node_ingredients = []\n",
    "for line in reader:\n",
    "    node_ingredients.append(line)\n",
    "f.close()    \n",
    "print(\"node_ingredients\", len(node_ingredients))\n",
    "num_ing = len(node_ingredients)\n",
    "\n",
    "# f = open('validation_classification_question.csv', 'r', encoding='utf-8')\n",
    "# reader = csv.reader(f)\n",
    "# val_cls_q = []\n",
    "# for line in reader:\n",
    "#     val_cls_q.append(line)\n",
    "# f.close()\n",
    "# print(\"val_cls_q\", len(val_cls_q))\n",
    "\n",
    "# f = open('validation_classification_answer.csv', 'r', encoding='utf-8')\n",
    "# reader = csv.reader(f)\n",
    "# val_cls_a = []\n",
    "# for line in reader:\n",
    "#     val_cls_a.append(line)\n",
    "# f.close()\n",
    "# print(\"val_cls_a\", len(val_cls_a))\n",
    "\n",
    "f = open('validation_completion_question.csv', 'r', encoding='utf-8')\n",
    "reader = csv.reader(f)\n",
    "val_cpt_q = []\n",
    "for line in reader:\n",
    "    val_cpt_q.append(line)\n",
    "f.close()\n",
    "print(\"val_cpt_q\", len(val_cpt_q))\n",
    "\n",
    "f = open('validation_completion_answer.csv', 'r', encoding='utf-8')\n",
    "reader = csv.reader(f)\n",
    "val_cpt_a = []\n",
    "for line in reader:\n",
    "    val_cpt_a.append(line)\n",
    "f.close()\n",
    "print(\"val_cpt_a\", len(val_cpt_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426ac4d",
   "metadata": {},
   "source": [
    "## Completion Task : Method 0\n",
    "find neighbors of largest weight (no use of embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a18ffcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714\n",
      "355816\n"
     ]
    }
   ],
   "source": [
    "# make graph of ingredients\n",
    "G = nx.Graph()\n",
    "for i in range(len(node_ingredients)):\n",
    "    G.add_node(str(i))\n",
    "    \n",
    "print(G.number_of_nodes())\n",
    "\n",
    "for data in trains:\n",
    "    for i in range(len(data) - 2):\n",
    "        for j in range(i+1, len(data) - 1):\n",
    "            if G.has_edge(data[i], data[j]):\n",
    "                G[data[i]][data[j]]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(data[i], data[j], weight=1)\n",
    "\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24654837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7848it [01:20, 97.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  6.167176350662589 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i, data in tqdm(enumerate(val_cpt_q)):\n",
    "#     print(data)\n",
    "    weight_dict = {}\n",
    "    for node in data:\n",
    "        for adv, w in G.adj[node].items():\n",
    "            if adv in weight_dict.keys():\n",
    "                weight_dict[adv] += w['weight']\n",
    "            else:\n",
    "                weight_dict[adv] = w['weight']\n",
    "    for node in data:\n",
    "        if node in weight_dict.keys():\n",
    "            del weight_dict[node]\n",
    "    \n",
    "    weight_dict = sorted(weight_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "    \n",
    "#     print(weight_dict)\n",
    "\n",
    "    if weight_dict[0][0] == val_cpt_a[i][0]:\n",
    "        acc += 1\n",
    "        \n",
    "print(\"accuracy: \", acc / len(val_cpt_q) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc083d",
   "metadata": {},
   "source": [
    "## Completion Task : Method 1\n",
    "cosine similarity of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f2aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(X,y):\n",
    "    return np.dot(X, y) / (np.linalg.norm(X, axis=1) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "354a9b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb1q10 (6714, 64)\n"
     ]
    }
   ],
   "source": [
    "# prepare embeddings as numpy\n",
    "f = open('Embedding/Embp1q10.csv', 'r', encoding='utf-8')\n",
    "reader = csv.reader(f)\n",
    "f.readline()\n",
    "embp1q10 = np.zeros((6714, 64))\n",
    "for line in reader:\n",
    "    i = int(line[1])\n",
    "    j = 0\n",
    "    for node in line[2][2:-1].split(' '):\n",
    "        if node != '':\n",
    "            embp1q10[i][j] = float(node.strip())\n",
    "            j += 1\n",
    "    \n",
    "f.close()\n",
    "print(\"emb1q10\", embp1q10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "368ceaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7848it [00:07, 1093.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.6116207951070336 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# method 1-1: similarity of average\n",
    "\n",
    "acc = 0\n",
    "for i, data in tqdm(enumerate(val_cpt_q)):\n",
    "    nodes = [int(node) for node in data]\n",
    "    avg_node = np.average(embp1q10[nodes][:], axis=0)\n",
    "    sims = cos_sim(embp1q10, avg_node)\n",
    "    ranks = np.argsort(-sims)\n",
    "        \n",
    "    target = int(val_cpt_a[i][0])\n",
    "    \n",
    "#     print(\"input: \", data)\n",
    "#     print(\"target: \", target)\n",
    "#     print(\"estimation: \", ranks[0:10])\n",
    "#     print(\"rank: \", np.where(ranks == target))\n",
    "#     print(\"\")\n",
    "\n",
    "    j = 0\n",
    "    while ranks[j] in data:\n",
    "        j += 1\n",
    "        \n",
    "    estimation = ranks[j]\n",
    "    \n",
    "    if estimation == target:\n",
    "        acc += 1\n",
    "        \n",
    "print(\"accuracy: \", acc / len(val_cpt_q) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ab69079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/lym7505/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "7848it [00:18, 429.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5224260958205913 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# method 1-2: average of similarity\n",
    "\n",
    "acc = 0\n",
    "for i, data in tqdm(enumerate(val_cpt_q)):\n",
    "    nodes = [int(node) for node in data]\n",
    "    sims = []\n",
    "    for node in nodes:\n",
    "        sims.append(cos_sim(embp1q10, embp1q10[node]))\n",
    "    sims = np.stack(sims, axis=0)\n",
    "    avg_sim = np.average(sims, axis=0)\n",
    "    ranks = np.argsort(-avg_sim)\n",
    "        \n",
    "    target = int(val_cpt_a[i][0])\n",
    "    \n",
    "    j = 0\n",
    "    while ranks[j] in data:\n",
    "        j += 1\n",
    "        \n",
    "    estimation = ranks[j]\n",
    "    \n",
    "    if estimation == target:\n",
    "        acc += 1\n",
    "        \n",
    "print(\"accuracy: \", acc / len(val_cpt_q) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08089a",
   "metadata": {},
   "source": [
    "## Completion Task : Method 2\n",
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478baee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainset(dataset):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for data in tqdm(dataset):\n",
    "        nodes = [int(node) for node in data[:-1]]\n",
    "        x = np.zeros((len(nodes), num_ing))\n",
    "        y = np.zeros(len(nodes))\n",
    "        for i, node in enumerate(nodes):\n",
    "            other_nodes = [nd for nd in nodes if nd != node]\n",
    "            for other in other_nodes:\n",
    "                x[i][other] = 1\n",
    "                y[i] = node\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    train_x = np.concatenate(xs, axis=0)\n",
    "    train_y = np.concatenate(ys, axis=0)\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e8180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 23547/23547 [00:05<00:00, 4207.67it/s]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = make_trainset(trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "07e7e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 45.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10297918006462584"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='l2', max_iter=10, verbose=True).fit(train_x, train_y)\n",
    "clf.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c58bad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_valset(question, answer):\n",
    "    val_x = np.zeros((len(question), num_ing))\n",
    "    val_y = np.zeros(len(question))\n",
    "    for i in range(len(question)):\n",
    "        nodes = [int(node) for node in question[i]]\n",
    "        target = int(answer[i][0])\n",
    "        for node in nodes:\n",
    "            val_x[i][node] = 1\n",
    "        val_y[i] = target\n",
    "        \n",
    "    return val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1a174a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x, val_y = make_valset(val_cpt_q, val_cpt_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cbd9aba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09174311926605505"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00af77d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2813, 3146, 3229, 3885, 4379, 4390, 5250, 5456, 6187]\n",
      "(9, 6714)\n",
      "2813 [3146, 3229, 3885, 4379, 4390, 5250, 5456, 6187]\n",
      "3146 [2813, 3229, 3885, 4379, 4390, 5250, 5456, 6187]\n",
      "3229 [2813, 3146, 3885, 4379, 4390, 5250, 5456, 6187]\n",
      "3885 [2813, 3146, 3229, 4379, 4390, 5250, 5456, 6187]\n",
      "4379 [2813, 3146, 3229, 3885, 4390, 5250, 5456, 6187]\n",
      "4390 [2813, 3146, 3229, 3885, 4379, 5250, 5456, 6187]\n",
      "5250 [2813, 3146, 3229, 3885, 4379, 4390, 5456, 6187]\n",
      "5456 [2813, 3146, 3229, 3885, 4379, 4390, 5250, 6187]\n",
      "6187 [2813, 3146, 3229, 3885, 4379, 4390, 5250, 5456]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c3/k7l8lkmx2lx29mdz272x1nz40000gn/T/ipykernel_5134/4107456411.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m train_loader = torch.utils.data.DataLoader(\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mTrainDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m     94\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# 0,1,2 => (1100 - 0010), (0110 - 1000), (1010 - 0100)\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_list, transform):\n",
    "        self.data = []\n",
    "        self.label = [] # missing ingredient\n",
    "        self.transform = transform\n",
    "        \n",
    "        for data in train_list[0:1]:\n",
    "            nodes = [int(node) for node in data[0:-1]]\n",
    "            print(nodes)\n",
    "            np_data = np.zeros((len(nodes), num_ing))\n",
    "            np_label = np.zeros((len(nodes), num_ing))\n",
    "            print(np_data.shape)\n",
    "            for mis_node in nodes:\n",
    "                other_node = [node for node in nodes if node!=mis_node]\n",
    "                print(mis_node, other_node)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.label[idx]\n",
    "        data = self.transform(data)\n",
    "        label = self.transform(label)\n",
    "        return data, label\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TrainDataset(trains, transform=torchvision.transforms.ToTensor()), \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for i, (data, label) in enumerate(train_loader):\n",
    "    print(i, data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9177ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
